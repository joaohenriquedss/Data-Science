{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reg_linear_simples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqcZj3z-VVtW",
        "colab_type": "text"
      },
      "source": [
        "## 1. Rode o mesmo programa nos dados contendo anos de escolaridade (primeira coluna) versus salário (segunda coluna)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qMSuTyDVhfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from numpy import *\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN9XVdbuUSZQ",
        "colab_type": "code",
        "outputId": "b38572db-7555-4837-9030-b89bdbd618a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#The optimal values of m and b can be actually calculated with way less effort than doing a linear regression. \n",
        "#this is just to demonstrate gradient descent\n",
        "\n",
        "\n",
        "# y = mx + b\n",
        "def compute_error_for_line_given_points(b, m, points):\n",
        "    totalError = 0\n",
        "    for i in range(0, len(points)):\n",
        "        x = points[i, 0]\n",
        "        y = points[i, 1]\n",
        "        totalError += (y - (m * x + b)) ** 2\n",
        "    return totalError\n",
        "\n",
        "def step_gradient(b_current, m_current, points, learningRate):\n",
        "    b_gradient = 0\n",
        "    m_gradient = 0\n",
        "    N = float(len(points))\n",
        "    for i in range(0, len(points)):\n",
        "        x = points[i, 0]\n",
        "        y = points[i, 1]\n",
        "        b_gradient += -(2) * (y - ((m_current * x) + b_current))\n",
        "        m_gradient += -(2) * x * (y - ((m_current * x) + b_current))\n",
        "    new_b = b_current - (learningRate * b_gradient)\n",
        "    new_m = m_current - (learningRate * m_gradient)\n",
        "    return [new_b, new_m]\n",
        "\n",
        "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
        "    b = starting_b\n",
        "    m = starting_m\n",
        "    for i in range(num_iterations):\n",
        "        b, m = step_gradient(b, m, array(points), learning_rate)        \n",
        "       \n",
        "    return [b, m]\n",
        "\n",
        "def run():\n",
        "    points = genfromtxt(\"income.csv\", delimiter=\",\")\n",
        "    learning_rate = 0.0001\n",
        "    initial_b = 0 # initial y-intercept guess\n",
        "    initial_m = 0 # initial slope guess\n",
        "    num_iterations = 1000\n",
        "    print (\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
        "    print (\"Running...\")\n",
        "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
        "    \n",
        "    print (\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting gradient descent at b = 0, m = 0, error = 88399.03491138059\n",
            "Running...\n",
            "After 1000 iterations b = -9.647708875888306, m = 3.8256361905811556, error = 2166.0903089213266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruyq8K9lc9TF",
        "colab_type": "text"
      },
      "source": [
        "## 2.Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente. Gere um plot mostrando o RSS por interação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvBhBBCSdqAM",
        "colab_type": "code",
        "outputId": "6801f5b5-0596-43d4-816d-c84c1db94f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "def modified(points, starting_b, starting_m, learning_rate, num_iterations):\n",
        "    b = starting_b\n",
        "    m = starting_m\n",
        "    result = []\n",
        "    for i in range(num_iterations):\n",
        "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
        "        result.append(compute_error_for_line_given_points(b,m,points))\n",
        "    return result\n",
        "  \n",
        "def run_rss():\n",
        "    points = genfromtxt(\"income.csv\", delimiter=\",\")\n",
        "    learning_rate = 0.0001\n",
        "    initial_b = 0 # initial y-intercept guess\n",
        "    initial_m = 0 # initial slope guess\n",
        "    num_iterations = 1000    \n",
        "    result = modified(points, initial_b, initial_m, learning_rate, num_iterations)\n",
        "    return result\n",
        "\n",
        "plt.plot(run_rss(),color=\"red\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f2760ad8470>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YYKxSwYnwFN",
        "colab_type": "text"
      },
      "source": [
        "##3 O que acontece com o RSS ao longo das iterações (aumenta ou diminui) se você usar 1000 iterações e um learning_rate (tamanho do passo do gradiente) de 0.001? Por que você acha que isso acontece?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j09EOdmKhrUr",
        "colab_type": "text"
      },
      "source": [
        "###**Diminui :** \n",
        "* O tamanho do learninge_rate influência diretamente no tamanho dos passos que daremos no vertice para o ponto minimo\n",
        "* Nesse caso a taxa de aprendizado é alta por consequência os passos serão cada vez mais longe do vertice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B8TU11i-pzy",
        "colab_type": "text"
      },
      "source": [
        "##4. Teste valores diferentes do número de iterações e learning_rate até que w0 e w1 sejam aproximadamente iguais a -39 e 5 respectivamente. Reporte os valores do número de iterações e learning_rate usados para atingir esses valores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZBQYjxhA2hO",
        "colab_type": "code",
        "outputId": "95d6d16e-16de-4c25-ec93-260ffc851a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def run():\n",
        "    points = genfromtxt(\"income.csv\", delimiter=\",\")\n",
        "    learning_rate = 0.0001\n",
        "    initial_b = 0 # initial y-intercept guess\n",
        "    initial_m = 0 # initial slope guess\n",
        "    num_iterations = 16555\n",
        "    print (\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
        "    print (\"Running...\")\n",
        "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
        "    \n",
        "    print (\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting gradient descent at b = 0, m = 0, error = 88399.03491138059\n",
            "Running...\n",
            "After 16555 iterations b = -39.094306410758456, m = 5.578531993547336, error = 895.0418212351153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7ZrcO-fEB3Y",
        "colab_type": "text"
      },
      "source": [
        "Com **16555** interação e com **0.0001** de learning_rate. Chegamos a um valor aproximado de **-39** e **5**\n",
        "* w0 = -39.094306410758456\n",
        "* w1 =  5.578531993547336"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjIhQ3xK-u5m",
        "colab_type": "text"
      },
      "source": [
        "##5.O algoritmo do vídeo usa o número de iterações como critério de parada. Mude o algoritmo para considerar um critério de tolerância que é comparado ao tamanho do gradiente (como no algoritmo dos slides apresentados em sala). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWYPs5FnhEnv",
        "colab_type": "code",
        "outputId": "cd482ce8-3ff9-46e1-bcaf-f2b589d3cdd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "import time\n",
        "\n",
        "def step_gradient(b_current, m_current, points, learningRate):\n",
        "    b_gradient = 0\n",
        "    m_gradient = 0\n",
        "    N = float(len(points))\n",
        "    for i in range(0, len(points)):\n",
        "        x = points[i, 0]\n",
        "        y = points[i, 1]\n",
        "        b_gradient += -(2) * (y - ((m_current * x) + b_current))\n",
        "        m_gradient += -(2) * x * (y - ((m_current * x) + b_current))\n",
        "    new_b = b_current - (learningRate * b_gradient)\n",
        "    new_m = m_current - (learningRate * m_gradient)\n",
        "    return [new_b, new_m,b_gradient,m_gradient]\n",
        "\n",
        "def gradient_descent_runner(points, starting_b, starting_m, learning_rate ):\n",
        "    b = starting_b\n",
        "    m = starting_m\n",
        "    num_iterations = 0\n",
        "    b_gradient = 1\n",
        "    m_gradient = 0\n",
        "    while( abs( b_gradient-m_gradient ) > 0.01):\n",
        "        b, m ,b_gradient,m_gradient= step_gradient(b, m, array(points), learning_rate)   \n",
        "        num_iterations+=1\n",
        "    return [b, m,num_iterations]\n",
        "\n",
        "def run():\n",
        "    points = genfromtxt(\"income.csv\", delimiter=\",\")\n",
        "    learning_rate = 0.0001\n",
        "    initial_b = 0 # initial y-intercept guess\n",
        "    initial_m = 0 # initial slope guess\n",
        "    \n",
        "    print (\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
        "    print (\"Running...\")\n",
        "    inicio = time.clock()\n",
        "    [b, m,num_iterations] = gradient_descent_runner(points, initial_b, initial_m, learning_rate)\n",
        "    final = time.clock()\n",
        "    t = final - inicio\n",
        "    print (\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
        "    print(\"Time\" , t)\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting gradient descent at b = 0, m = 0, error = 88399.03491138059\n",
            "Running...\n",
            "After 32912 iterations b = -39.44295011811842, m = 5.59928604129897, error = 894.864501622754\n",
            "Time 1.348910999999994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGFXTHqN-0FP",
        "colab_type": "text"
      },
      "source": [
        "##6.Ache um valor de tolerância que se aproxime dos valores dos parâmetros do item 4 acima. Que valor foi esse?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at9LuUzmeeg0",
        "colab_type": "text"
      },
      "source": [
        "**A questão 5 consegue w0 e w1 sejam aproximadamente iguais a -39 e 5 respectivamente**\n",
        "\n",
        "O valor de tolerância usado é **0.01**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MppMXsg-5El",
        "colab_type": "text"
      },
      "source": [
        "##7. Implemente a forma fechada (equações normais) de calcular os coeficientes de regressão (vide algoritmo nos slides). Compare o tempo de processamento com o gradiente descendente considerando sua solução do item 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uliAhT2vd6P",
        "colab_type": "code",
        "outputId": "2e9d1aab-bfb6-4113-b15a-7b17d44ef31d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "\n",
        "def gradient_descent_runner(points):\n",
        "    meanX = mean(points[:,0])\n",
        "    meanY = mean(points[:,1])\n",
        "    dividend = 0\n",
        "    divider = 0\n",
        "    for i in range(len(points)):\n",
        "      x = points[i, 0]\n",
        "      y = points[i, 1]\n",
        "      dividend += (x - meanX)*(y - meanY)\n",
        "      divider += (x - meanX)**2\n",
        "    new_m = dividend/divider\n",
        "    new_b = meanY - (new_m*meanX)\n",
        "    return [new_b, new_m]\n",
        "\n",
        "\n",
        "def run():\n",
        "    \n",
        "    points = genfromtxt(\"income.csv\", delimiter=\",\")\n",
        "    print(\"Running...\")\n",
        "    inicio = time.clock()\n",
        "    [b, m] = gradient_descent_runner(points)\n",
        "    final = time.clock()\n",
        "    t = final - inicio \n",
        "    print(\"b = {}, m = {}, error = {}\".format(b, m, compute_error_for_line_given_points(b, m, points)))\n",
        "    print(\"Time\",t)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running...\n",
            "b = -39.446256679096194, m = 5.59948287411992, error = 894.8644859701869\n",
            "Time 0.00013299999999816237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBkQybaaw_v9",
        "colab_type": "text"
      },
      "source": [
        "###Como podemos observar o Time da Questão 6 foi *1.348910999999994* e na Questão 7 foi apenas *0.00013299999999816237* Uma diferença consideravelmente alta"
      ]
    }
  ]
}